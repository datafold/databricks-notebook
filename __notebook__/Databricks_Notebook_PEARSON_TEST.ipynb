{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing databricks-notebook from /Users/sergeyklinov/databricks-notebook\n",
      "Obtaining file:///Users/sergeyklinov/databricks-notebook\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: python-dotenv>=1.2.1 in /Users/sergeyklinov/databricks-notebook/.venv/lib/python3.12/site-packages (from databricks-notebook==0.1.0) (1.2.1)\n",
      "Building wheels for collected packages: databricks-notebook\n",
      "  Building editable for databricks-notebook (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for databricks-notebook: filename=databricks_notebook-0.1.0-0.editable-py3-none-any.whl size=2939 sha256=c97e1f72ab6a40e112e24b1a4740613802105aa28812cd18245f7ac8743c476d\n",
      "  Stored in directory: /private/var/folders/3y/p4yqdnw167xfr84r44_t60lh0000gn/T/pip-ephem-wheel-cache-ji9la71b/wheels/d6/fe/61/e1ee441d5c3d6bacd1e078d8335cf494301163e0f54e0a9d49\n",
      "Successfully built databricks-notebook\n",
      "Installing collected packages: databricks-notebook\n",
      "  Attempting uninstall: databricks-notebook\n",
      "    Found existing installation: databricks-notebook 0.1.0\n",
      "    Uninstalling databricks-notebook-0.1.0:\n",
      "      Successfully uninstalled databricks-notebook-0.1.0\n",
      "Successfully installed databricks-notebook-0.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Install dependencies\n",
    "LOCAL_DATABRICKS_NOTEBOOK_PATH = os.getenv('LOCAL_DATABRICKS_NOTEBOOK_PATH')\n",
    "if LOCAL_DATABRICKS_NOTEBOOK_PATH and pathlib.Path(LOCAL_DATABRICKS_NOTEBOOK_PATH).exists():\n",
    "    print(f\"Installing databricks-notebook from {LOCAL_DATABRICKS_NOTEBOOK_PATH}\")\n",
    "    %pip install --editable \"{LOCAL_DATABRICKS_NOTEBOOK_PATH}\"\n",
    "else:\n",
    "    print(\"Installing databricks-notebook from git\")\n",
    "    %pip install git+https://github.com/datafold/databricks-notebook.git\n",
    "\n",
    "# Restart to make dependencies available\n",
    "# %restart_python on databricks notebook\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_token = \"my_secret_token\" # do not change\n",
    "host=\"https://sergey.st.datafold.io\"\n",
    "identity = None\n",
    "\n",
    "# We collect basic identity information to help track and resolve any issues\n",
    "# with SQL translation and provide you with the best experience. This data is\n",
    "# used internally by Datafold only and helps us:\n",
    "# - Diagnose translation errors specific to your workspace configuration\n",
    "# - Improve translation quality based on real usage patterns\n",
    "# - Provide better support when you need assistance\n",
    "#\n",
    "# If you prefer not to share certain information, you can comment out specific\n",
    "# fields below or remove this entire code block. The tool will still work, but\n",
    "# we may have limited ability to help troubleshoot issues.\n",
    "\n",
    "# def get_context_info():\n",
    "#     context = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "#     return {\n",
    "#         'workspace_id': context.workspaceId().get(),\n",
    "#         'workspace_url': context.browserHostName().get(),\n",
    "#         'cluster_id': context.clusterId().get(),\n",
    "#         'notebook_path': context.notebookPath().get(),\n",
    "#         'user': context.userName().get()\n",
    "#     }\n",
    "\n",
    "# identity = get_context_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading queries from: /Users/sergeyklinov/dma/dma-pearson-assessment/included_queries.csv\n",
      "Found 4285 queries to translate\n",
      "Processing in chunks of 5\n",
      "\n",
      "⚠️  TEST MODE ENABLED - Processing only first 2 chunks (10 queries max)\n",
      "   Set TEST_MODE = False to process all queries\n",
      "\n",
      "Output file initialized: /Users/sergeyklinov/dma/dma-pearson-assessment/report.csv\n",
      "\n",
      "=== Processing queries 1 to 5 of 4285 ===\n",
      "Translating 5 queries...\n",
      "✓ Organization created with id 13\n",
      "✓ Translation Project created with id 10014.\n",
      "✓ Uploaded queries to translate.\n",
      "✓ Started translation with id 1cd216cc-a405-404b-8d7f-cdf972d14474\n",
      "✓ Translation completed with status: done\n",
      "Translation completed for this chunk!\n",
      "✓ Chunk results written to report (5/4285 queries processed)\n",
      "\n",
      "=== Processing queries 6 to 10 of 4285 ===\n",
      "Translating 5 queries...\n",
      "✓ Translation Project created with id 10015.\n",
      "✓ Uploaded queries to translate.\n",
      "✓ Started translation with id 474f044b-ef4c-47f0-94a4-eb2469dcbc60\n",
      "✓ Translation completed with status: done\n",
      "Translation completed for this chunk!\n",
      "✓ Chunk results written to report (10/4285 queries processed)\n",
      "\n",
      "\n",
      "⚠️  TEST MODE: Stopping after 2 chunks\n",
      "==================================================\n",
      "✓ Test run completed! Processed 2 chunks (10 queries max)\n",
      "   To process all queries, set TEST_MODE = False\n",
      "✓ Report file: /Users/sergeyklinov/dma/dma-pearson-assessment/report.csv\n",
      "\n",
      "=== Translation Summary ===\n",
      "Queries processed: 10\n",
      "Successfully translated: 0\n",
      "Failed: 0\n",
      "Other: 10\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "from pathlib import Path\n",
    "from databricks_notebook import translate_queries_and_get_results\n",
    "\n",
    "# Configuration\n",
    "CHUNK_SIZE = 5  # Process 100 queries at a time\n",
    "TEST_MODE = True  # Set to False to process all queries\n",
    "MAX_CHUNKS_TEST = 2  # Only process first 2 chunks in test mode\n",
    "\n",
    "input_csv_path = Path.home() / \"dma\" / \"dma-pearson-assessment\" / \"included_queries.csv\"\n",
    "output_csv_path = Path.home() / \"dma\" / \"dma-pearson-assessment\" / \"report.csv\"\n",
    "\n",
    "print(f\"Reading queries from: {input_csv_path}\")\n",
    "\n",
    "# Read all queries from CSV\n",
    "queries_data = []\n",
    "with open(input_csv_path, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        queries_data.append({\n",
    "            'query_hash': row['QueryHash'],\n",
    "            'query_text': row['QueryText']\n",
    "        })\n",
    "\n",
    "total_queries = len(queries_data)\n",
    "print(f\"Found {total_queries} queries to translate\")\n",
    "print(f\"Processing in chunks of {CHUNK_SIZE}\")\n",
    "\n",
    "if TEST_MODE:\n",
    "    print(f\"\\n⚠️  TEST MODE ENABLED - Processing only first {MAX_CHUNKS_TEST} chunks ({MAX_CHUNKS_TEST * CHUNK_SIZE} queries max)\")\n",
    "    print(f\"   Set TEST_MODE = False to process all queries\\n\")\n",
    "\n",
    "# Initialize or clear the output CSV file with headers\n",
    "with open(output_csv_path, 'w', newline='', encoding='utf-8') as f:\n",
    "    fieldnames = ['query_hash', 'asset_name', 'original_query', 'translation_status', 'translation']\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "print(f\"Output file initialized: {output_csv_path}\\n\")\n",
    "\n",
    "# Process queries in chunks\n",
    "success_count = 0\n",
    "failed_count = 0\n",
    "other_count = 0\n",
    "chunks_processed = 0\n",
    "\n",
    "for chunk_start in range(0, total_queries, CHUNK_SIZE):\n",
    "    # Stop after MAX_CHUNKS_TEST chunks if in test mode\n",
    "    if TEST_MODE and chunks_processed >= MAX_CHUNKS_TEST:\n",
    "        print(f\"\\n⚠️  TEST MODE: Stopping after {chunks_processed} chunks\")\n",
    "        break\n",
    "    \n",
    "    chunk_end = min(chunk_start + CHUNK_SIZE, total_queries)\n",
    "    chunk_queries_data = queries_data[chunk_start:chunk_end]\n",
    "    \n",
    "    print(f\"=== Processing queries {chunk_start + 1} to {chunk_end} of {total_queries} ===\")\n",
    "    \n",
    "    # Extract query texts for this chunk\n",
    "    queries_to_translate = [q['query_text'] for q in chunk_queries_data]\n",
    "    \n",
    "    # Translate this chunk\n",
    "    print(f\"Translating {len(queries_to_translate)} queries...\")\n",
    "    translation_results = translate_queries_and_get_results(\n",
    "        queries_to_translate, \n",
    "        org_token, \n",
    "        identity, \n",
    "        host\n",
    "    )\n",
    "    \n",
    "    print(\"Translation completed for this chunk!\")\n",
    "    \n",
    "    # Prepare report data for this chunk\n",
    "    chunk_report_rows = []\n",
    "    translated_models = translation_results.get('translated_models', [])\n",
    "    \n",
    "    for i, query_data in enumerate(chunk_queries_data):\n",
    "        # Match the query with its translation result by index\n",
    "        if i < len(translated_models):\n",
    "            model = translated_models[i]\n",
    "            status = model.get('translation_status', '')\n",
    "            \n",
    "            report_row = {\n",
    "                'query_hash': query_data['query_hash'],\n",
    "                'asset_name': model.get('asset_name', ''),\n",
    "                'original_query': query_data['query_text'],\n",
    "                'translation_status': status,\n",
    "                'translation': model.get('target_sql', '')\n",
    "            }\n",
    "            \n",
    "            # Update counters\n",
    "            if status == 'success':\n",
    "                success_count += 1\n",
    "            elif status == 'failed':\n",
    "                failed_count += 1\n",
    "            else:\n",
    "                other_count += 1\n",
    "        else:\n",
    "            # In case there's a mismatch\n",
    "            report_row = {\n",
    "                'query_hash': query_data['query_hash'],\n",
    "                'asset_name': '',\n",
    "                'original_query': query_data['query_text'],\n",
    "                'translation_status': 'not_translated',\n",
    "                'translation': ''\n",
    "            }\n",
    "            other_count += 1\n",
    "        \n",
    "        chunk_report_rows.append(report_row)\n",
    "    \n",
    "    # Append this chunk's results to the CSV file\n",
    "    with open(output_csv_path, 'a', newline='', encoding='utf-8') as f:\n",
    "        fieldnames = ['query_hash', 'asset_name', 'original_query', 'translation_status', 'translation']\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writerows(chunk_report_rows)\n",
    "    \n",
    "    chunks_processed += 1\n",
    "    print(f\"✓ Chunk results written to report ({chunk_end}/{total_queries} queries processed)\\n\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "if TEST_MODE and chunks_processed >= MAX_CHUNKS_TEST:\n",
    "    print(f\"✓ Test run completed! Processed {chunks_processed} chunks ({chunks_processed * CHUNK_SIZE} queries max)\")\n",
    "    print(f\"   To process all queries, set TEST_MODE = False\")\n",
    "else:\n",
    "    print(\"✓ All translations completed!\")\n",
    "print(f\"✓ Report file: {output_csv_path}\")\n",
    "print(\"\\n=== Translation Summary ===\")\n",
    "print(f\"Queries processed: {chunks_processed * CHUNK_SIZE if TEST_MODE else total_queries}\")\n",
    "print(f\"Successfully translated: {success_count}\")\n",
    "print(f\"Failed: {failed_count}\")\n",
    "print(f\"Other: {other_count}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Live Translations for Databricks",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
