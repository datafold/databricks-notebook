{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing databricks-notebook from /Users/sergeyklinov/databricks-notebook\n",
      "Obtaining file:///Users/sergeyklinov/databricks-notebook\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: python-dotenv>=1.2.1 in /Users/sergeyklinov/databricks-notebook/.venv/lib/python3.12/site-packages (from databricks-notebook==0.1.0) (1.2.1)\n",
      "Building wheels for collected packages: databricks-notebook\n",
      "  Building editable for databricks-notebook (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for databricks-notebook: filename=databricks_notebook-0.1.0-0.editable-py3-none-any.whl size=2939 sha256=8d2920d0e775388bb44d79c31bd66dbbc8267dfd813d1b1b94e4af57c4689a79\n",
      "  Stored in directory: /private/var/folders/3y/p4yqdnw167xfr84r44_t60lh0000gn/T/pip-ephem-wheel-cache-sb9nikem/wheels/d6/fe/61/e1ee441d5c3d6bacd1e078d8335cf494301163e0f54e0a9d49\n",
      "Successfully built databricks-notebook\n",
      "Installing collected packages: databricks-notebook\n",
      "  Attempting uninstall: databricks-notebook\n",
      "    Found existing installation: databricks-notebook 0.1.0\n",
      "    Uninstalling databricks-notebook-0.1.0:\n",
      "      Successfully uninstalled databricks-notebook-0.1.0\n",
      "Successfully installed databricks-notebook-0.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Install dependencies\n",
    "LOCAL_DATABRICKS_NOTEBOOK_PATH = os.getenv('LOCAL_DATABRICKS_NOTEBOOK_PATH')\n",
    "if LOCAL_DATABRICKS_NOTEBOOK_PATH and pathlib.Path(LOCAL_DATABRICKS_NOTEBOOK_PATH).exists():\n",
    "    print(f\"Installing databricks-notebook from {LOCAL_DATABRICKS_NOTEBOOK_PATH}\")\n",
    "    %pip install --editable \"{LOCAL_DATABRICKS_NOTEBOOK_PATH}\"\n",
    "else:\n",
    "    print(\"Installing databricks-notebook from git\")\n",
    "    %pip install git+https://github.com/datafold/databricks-notebook.git\n",
    "\n",
    "# Restart to make dependencies available\n",
    "# %restart_python on databricks notebook\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_token = \"my_secret_token\" # do not change\n",
    "host=\"https://sergey.st.datafold.io\"\n",
    "identity = None\n",
    "\n",
    "# We collect basic identity information to help track and resolve any issues\n",
    "# with SQL translation and provide you with the best experience. This data is\n",
    "# used internally by Datafold only and helps us:\n",
    "# - Diagnose translation errors specific to your workspace configuration\n",
    "# - Improve translation quality based on real usage patterns\n",
    "# - Provide better support when you need assistance\n",
    "#\n",
    "# If you prefer not to share certain information, you can comment out specific\n",
    "# fields below or remove this entire code block. The tool will still work, but\n",
    "# we may have limited ability to help troubleshoot issues.\n",
    "\n",
    "# def get_context_info():\n",
    "#     context = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "#     return {\n",
    "#         'workspace_id': context.workspaceId().get(),\n",
    "#         'workspace_url': context.browserHostName().get(),\n",
    "#         'cluster_id': context.clusterId().get(),\n",
    "#         'notebook_path': context.notebookPath().get(),\n",
    "#         'user': context.userName().get()\n",
    "#     }\n",
    "\n",
    "# identity = get_context_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Database initialized: /Users/sergeyklinov/dma/dma-pearson-assessment/translations.db\n",
      "\n",
      "=== Current Database Stats ===\n",
      "Total queries in DB: 0\n",
      "Successfully translated: 0\n",
      "Failed: 0\n",
      "Other: 0\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# SQLite Database Setup\n",
    "# ========================================\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Database path\n",
    "db_path = Path.home() / \"dma\" / \"dma-pearson-assessment\" / \"translations.db\"\n",
    "\n",
    "def init_database():\n",
    "    \"\"\"Initialize the SQLite database with the translations table\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Create table if it doesn't exist\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS translations (\n",
    "            query_hash TEXT PRIMARY KEY,\n",
    "            asset_name TEXT,\n",
    "            original_query TEXT,\n",
    "            translation_status TEXT,\n",
    "            translation TEXT,\n",
    "            project_id INTEGER,\n",
    "            translation_id TEXT,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create index on status for faster queries\n",
    "    cursor.execute('''\n",
    "        CREATE INDEX IF NOT EXISTS idx_translation_status \n",
    "        ON translations(translation_status)\n",
    "    ''')\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(f\"‚úì Database initialized: {db_path}\")\n",
    "\n",
    "def save_translation_results(results, project_id=None, translation_id=None):\n",
    "    \"\"\"\n",
    "    Save or update translation results in the database\n",
    "    Uses UPSERT (INSERT OR REPLACE) to handle duplicates\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    current_time = datetime.now().isoformat()\n",
    "    \n",
    "    for result in results:\n",
    "        cursor.execute('''\n",
    "            INSERT INTO translations \n",
    "            (query_hash, asset_name, original_query, translation_status, \n",
    "             translation, project_id, translation_id, created_at, updated_at)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, \n",
    "                    COALESCE((SELECT created_at FROM translations WHERE query_hash = ?), ?),\n",
    "                    ?)\n",
    "            ON CONFLICT(query_hash) DO UPDATE SET\n",
    "                asset_name = excluded.asset_name,\n",
    "                original_query = excluded.original_query,\n",
    "                translation_status = excluded.translation_status,\n",
    "                translation = excluded.translation,\n",
    "                project_id = excluded.project_id,\n",
    "                translation_id = excluded.translation_id,\n",
    "                updated_at = excluded.updated_at\n",
    "        ''', (\n",
    "            result['query_hash'],\n",
    "            result['asset_name'],\n",
    "            result['original_query'],\n",
    "            result['translation_status'],\n",
    "            result['translation'],\n",
    "            project_id,\n",
    "            translation_id,\n",
    "            result['query_hash'],  # for COALESCE lookup\n",
    "            current_time,\n",
    "            current_time\n",
    "        ))\n",
    "    \n",
    "    conn.commit()\n",
    "    rows_affected = cursor.rowcount\n",
    "    conn.close()\n",
    "    \n",
    "    return rows_affected\n",
    "\n",
    "def get_translation_stats():\n",
    "    \"\"\"Get summary statistics from the database\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute('''\n",
    "        SELECT \n",
    "            COUNT(*) as total,\n",
    "            SUM(CASE WHEN translation_status = 'success' THEN 1 ELSE 0 END) as success,\n",
    "            SUM(CASE WHEN translation_status = 'failed' THEN 1 ELSE 0 END) as failed,\n",
    "            SUM(CASE WHEN translation_status NOT IN ('success', 'failed') THEN 1 ELSE 0 END) as other\n",
    "        FROM translations\n",
    "    ''')\n",
    "    \n",
    "    result = cursor.fetchone()\n",
    "    conn.close()\n",
    "    \n",
    "    return {\n",
    "        'total': result[0] or 0,\n",
    "        'success': result[1] or 0,\n",
    "        'failed': result[2] or 0,\n",
    "        'other': result[3] or 0\n",
    "    }\n",
    "\n",
    "def get_untranslated_queries(input_csv_path):\n",
    "    \"\"\"\n",
    "    Find queries from input CSV that haven't been translated yet\n",
    "    Returns list of query_hash values\n",
    "    \"\"\"\n",
    "    import csv\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Get all translated query hashes\n",
    "    cursor.execute('SELECT query_hash FROM translations')\n",
    "    translated_hashes = {row[0] for row in cursor.fetchall()}\n",
    "    conn.close()\n",
    "    \n",
    "    # Read all query hashes from input CSV\n",
    "    all_hashes = []\n",
    "    with open(input_csv_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            all_hashes.append(row['QueryHash'])\n",
    "    \n",
    "    # Find untranslated\n",
    "    untranslated = [h for h in all_hashes if h not in translated_hashes]\n",
    "    \n",
    "    return untranslated\n",
    "\n",
    "def export_to_csv(output_csv_path):\n",
    "    \"\"\"Export all translations from database to CSV file\"\"\"\n",
    "    import csv\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute('''\n",
    "        SELECT query_hash, asset_name, original_query, translation_status, translation\n",
    "        FROM translations\n",
    "        ORDER BY query_hash\n",
    "    ''')\n",
    "    \n",
    "    with open(output_csv_path, 'w', newline='', encoding='utf-8') as f:\n",
    "        fieldnames = ['query_hash', 'asset_name', 'original_query', 'translation_status', 'translation']\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for row in cursor.fetchall():\n",
    "            writer.writerow({\n",
    "                'query_hash': row[0],\n",
    "                'asset_name': row[1],\n",
    "                'original_query': row[2],\n",
    "                'translation_status': row[3],\n",
    "                'translation': row[4]\n",
    "            })\n",
    "    \n",
    "    conn.close()\n",
    "    print(f\"‚úì Exported to CSV: {output_csv_path}\")\n",
    "\n",
    "# Initialize the database\n",
    "init_database()\n",
    "\n",
    "# Show current stats\n",
    "stats = get_translation_stats()\n",
    "print(f\"\\n=== Current Database Stats ===\")\n",
    "print(f\"Total queries in DB: {stats['total']}\")\n",
    "print(f\"Successfully translated: {stats['success']}\")\n",
    "print(f\"Failed: {stats['failed']}\")\n",
    "print(f\"Other: {stats['other']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading queries from: /Users/sergeyklinov/dma/dma-pearson-assessment/included_queries.csv\n",
      "Found 4285 queries to translate\n",
      "‚ö†Ô∏è  OFFSET SET: Starting from query #101 (skipping first 100 queries)\n",
      "   Remaining queries to process: 4185\n",
      "Processing in chunks of 10\n",
      "\n",
      "‚ö†Ô∏è  TEST MODE ENABLED - Processing only first 5 chunks (50 queries max)\n",
      "   Set TEST_MODE = False to process all queries\n",
      "\n",
      "\n",
      "=== Database Status ===\n",
      "Already in DB: 65 queries\n",
      "  Success: 0, Failed: 0, Other: 65\n",
      "\n",
      "=== Processing queries 101 to 110 of 4285 (total in file) ===\n",
      "Translating 10 queries...\n",
      "‚úì Translation Project created with id 10035.\n",
      "‚úì Uploaded queries to translate.\n",
      "‚úì Started translation with id 5dba8abc-df31-4c26-9adb-4e99a4fe1591\n",
      "\n",
      "üìã Recovery Info (save these if connection fails):\n",
      "   Project ID: 10035\n",
      "   Translation ID: 5dba8abc-df31-4c26-9adb-4e99a4fe1591\n",
      "   Chunk Start Index: 100\n",
      "\n",
      "‚úì Translation completed with status: done\n",
      "Translation completed for this chunk!\n",
      "‚úì Chunk results saved to database (10 queries processed in this run)\n",
      "\n",
      "=== Processing queries 111 to 120 of 4285 (total in file) ===\n",
      "Translating 10 queries...\n",
      "‚úì Translation Project created with id 10036.\n",
      "‚úì Uploaded queries to translate.\n",
      "‚úì Started translation with id d3ce574c-eaf7-476d-a447-183afeabdb45\n",
      "\n",
      "üìã Recovery Info (save these if connection fails):\n",
      "   Project ID: 10036\n",
      "   Translation ID: d3ce574c-eaf7-476d-a447-183afeabdb45\n",
      "   Chunk Start Index: 110\n",
      "\n",
      "‚úì Translation completed with status: done\n",
      "Translation completed for this chunk!\n",
      "‚úì Chunk results saved to database (20 queries processed in this run)\n",
      "\n",
      "=== Processing queries 121 to 130 of 4285 (total in file) ===\n",
      "Translating 10 queries...\n",
      "‚úì Translation Project created with id 10037.\n",
      "‚úì Uploaded queries to translate.\n",
      "‚úì Started translation with id 7cb34d74-3e8b-49e1-9c47-5474feb5a1a0\n",
      "\n",
      "üìã Recovery Info (save these if connection fails):\n",
      "   Project ID: 10037\n",
      "   Translation ID: 7cb34d74-3e8b-49e1-9c47-5474feb5a1a0\n",
      "   Chunk Start Index: 120\n",
      "\n",
      "‚úì Translation completed with status: done\n",
      "Translation completed for this chunk!\n",
      "‚úì Chunk results saved to database (30 queries processed in this run)\n",
      "\n",
      "=== Processing queries 131 to 140 of 4285 (total in file) ===\n",
      "Translating 10 queries...\n",
      "‚úì Translation Project created with id 10038.\n",
      "‚úì Uploaded queries to translate.\n",
      "‚úì Started translation with id f852cfb7-846a-4f98-844a-5ea2407f971e\n",
      "\n",
      "üìã Recovery Info (save these if connection fails):\n",
      "   Project ID: 10038\n",
      "   Translation ID: f852cfb7-846a-4f98-844a-5ea2407f971e\n",
      "   Chunk Start Index: 130\n",
      "\n",
      "‚úì Translation completed with status: done\n",
      "Translation completed for this chunk!\n",
      "‚úì Chunk results saved to database (40 queries processed in this run)\n",
      "\n",
      "=== Processing queries 141 to 150 of 4285 (total in file) ===\n",
      "Translating 10 queries...\n",
      "‚úì Translation Project created with id 10039.\n",
      "‚úì Uploaded queries to translate.\n",
      "‚úì Started translation with id 0d9dfdb0-cf68-4f59-b1c7-f479cba315d9\n",
      "\n",
      "üìã Recovery Info (save these if connection fails):\n",
      "   Project ID: 10039\n",
      "   Translation ID: 0d9dfdb0-cf68-4f59-b1c7-f479cba315d9\n",
      "   Chunk Start Index: 140\n",
      "\n",
      "‚úì Translation completed with status: done\n",
      "Translation completed for this chunk!\n",
      "‚úì Chunk results saved to database (50 queries processed in this run)\n",
      "\n",
      "\n",
      "‚ö†Ô∏è  TEST MODE: Stopping after 5 chunks\n",
      "==================================================\n",
      "‚úì Test run completed! Processed 5 chunks (50 queries)\n",
      "   To process all queries, set TEST_MODE = False\n",
      "\n",
      "=== Final Database Stats ===\n",
      "Total queries in DB: 115\n",
      "Successfully translated: 0\n",
      "Failed: 0\n",
      "Other: 115\n",
      "\n",
      "üì§ Exporting results to CSV...\n",
      "‚úì Exported to CSV: /Users/sergeyklinov/dma/dma-pearson-assessment/report.csv\n",
      "‚úì Report file: /Users/sergeyklinov/dma/dma-pearson-assessment/report.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "from pathlib import Path\n",
    "from databricks_notebook import translate_queries, view_translation_results_as_dict, _get_current_api_key\n",
    "\n",
    "# Configuration\n",
    "CHUNK_SIZE = 10  # Process queries at a time\n",
    "TEST_MODE = True  # Set to False to process all queries\n",
    "MAX_CHUNKS_TEST = 5  # Only process first 2 chunks in test mode\n",
    "OFFSET = 100  # Start from this query number (0 = start from beginning)\n",
    "\n",
    "input_csv_path = Path.home() / \"dma\" / \"dma-pearson-assessment\" / \"included_queries.csv\"\n",
    "output_csv_path = Path.home() / \"dma\" / \"dma-pearson-assessment\" / \"report.csv\"\n",
    "\n",
    "print(f\"Reading queries from: {input_csv_path}\")\n",
    "\n",
    "# Read all queries from CSV\n",
    "queries_data = []\n",
    "with open(input_csv_path, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        queries_data.append({\n",
    "            'query_hash': row['QueryHash'],\n",
    "            'query_text': row['QueryText']\n",
    "        })\n",
    "\n",
    "total_queries = len(queries_data)\n",
    "print(f\"Found {total_queries} queries to translate\")\n",
    "\n",
    "# Apply offset\n",
    "if OFFSET > 0:\n",
    "    queries_data = queries_data[OFFSET:]\n",
    "    print(f\"‚ö†Ô∏è  OFFSET SET: Starting from query #{OFFSET + 1} (skipping first {OFFSET} queries)\")\n",
    "    print(f\"   Remaining queries to process: {len(queries_data)}\")\n",
    "\n",
    "print(f\"Processing in chunks of {CHUNK_SIZE}\")\n",
    "\n",
    "if TEST_MODE:\n",
    "    print(f\"\\n‚ö†Ô∏è  TEST MODE ENABLED - Processing only first {MAX_CHUNKS_TEST} chunks ({MAX_CHUNKS_TEST * CHUNK_SIZE} queries max)\")\n",
    "    print(f\"   Set TEST_MODE = False to process all queries\\n\")\n",
    "\n",
    "# Show database stats before starting\n",
    "stats = get_translation_stats()\n",
    "print(f\"\\n=== Database Status ===\")\n",
    "print(f\"Already in DB: {stats['total']} queries\")\n",
    "print(f\"  Success: {stats['success']}, Failed: {stats['failed']}, Other: {stats['other']}\\n\")\n",
    "\n",
    "# Get API key once at the beginning\n",
    "api_key = _get_current_api_key(org_token, host)\n",
    "\n",
    "# Process queries in chunks\n",
    "chunks_processed = 0\n",
    "queries_processed = 0\n",
    "\n",
    "for chunk_start in range(0, len(queries_data), CHUNK_SIZE):\n",
    "    # Stop after MAX_CHUNKS_TEST chunks if in test mode\n",
    "    if TEST_MODE and chunks_processed >= MAX_CHUNKS_TEST:\n",
    "        print(f\"\\n‚ö†Ô∏è  TEST MODE: Stopping after {chunks_processed} chunks\")\n",
    "        break\n",
    "    \n",
    "    chunk_end = min(chunk_start + CHUNK_SIZE, len(queries_data))\n",
    "    chunk_queries_data = queries_data[chunk_start:chunk_end]\n",
    "    \n",
    "    actual_start = OFFSET + chunk_start + 1\n",
    "    actual_end = OFFSET + chunk_end\n",
    "    print(f\"=== Processing queries {actual_start} to {actual_end} of {total_queries} (total in file) ===\")\n",
    "    \n",
    "    # Extract query texts for this chunk\n",
    "    queries_to_translate = [q['query_text'] for q in chunk_queries_data]\n",
    "    \n",
    "    # Translate this chunk\n",
    "    print(f\"Translating {len(queries_to_translate)} queries...\")\n",
    "    \n",
    "    try:\n",
    "        # Start translation and get IDs\n",
    "        project_id, translation_id = translate_queries(api_key, queries_to_translate, host)\n",
    "        \n",
    "        print(f\"\\nüìã Recovery Info (save these if connection fails):\")\n",
    "        print(f\"   Project ID: {project_id}\")\n",
    "        print(f\"   Translation ID: {translation_id}\")\n",
    "        print(f\"   Chunk Start Index: {OFFSET + chunk_start}\\n\")\n",
    "        \n",
    "        # Wait for and fetch results\n",
    "        translation_results = view_translation_results_as_dict(\n",
    "            api_key, \n",
    "            project_id, \n",
    "            translation_id, \n",
    "            host\n",
    "        )\n",
    "        \n",
    "        print(\"Translation completed for this chunk!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error during translation: {e}\")\n",
    "        print(f\"\\nüìã To recover, use the recovery cell with:\")\n",
    "        print(f\"   RECOVERY_PROJECT_ID = {project_id if 'project_id' in locals() else 'NOT_AVAILABLE'}\")\n",
    "        print(f\"   RECOVERY_TRANSLATION_ID = \\\"{translation_id if 'translation_id' in locals() else 'NOT_AVAILABLE'}\\\"\")\n",
    "        print(f\"   RECOVERY_CHUNK_START = {OFFSET + chunk_start}\")\n",
    "        print(f\"\\nStopping processing due to error.\\n\")\n",
    "        break\n",
    "    \n",
    "    # Prepare report data for this chunk\n",
    "    chunk_report_rows = []\n",
    "    translated_models = translation_results.get('translated_models', [])\n",
    "    \n",
    "    for i, query_data in enumerate(chunk_queries_data):\n",
    "        # Match the query with its translation result by index\n",
    "        if i < len(translated_models):\n",
    "            model = translated_models[i]\n",
    "            status = model.get('translation_status', '')\n",
    "            \n",
    "            report_row = {\n",
    "                'query_hash': query_data['query_hash'],\n",
    "                'asset_name': model.get('asset_name', ''),\n",
    "                'original_query': query_data['query_text'],\n",
    "                'translation_status': status,\n",
    "                'translation': model.get('target_sql', '')\n",
    "            }\n",
    "        else:\n",
    "            # In case there's a mismatch\n",
    "            report_row = {\n",
    "                'query_hash': query_data['query_hash'],\n",
    "                'asset_name': '',\n",
    "                'original_query': query_data['query_text'],\n",
    "                'translation_status': 'not_translated',\n",
    "                'translation': ''\n",
    "            }\n",
    "        \n",
    "        chunk_report_rows.append(report_row)\n",
    "    \n",
    "    # Save to SQLite database (upserts by query_hash)\n",
    "    save_translation_results(chunk_report_rows, project_id, translation_id)\n",
    "    \n",
    "    chunks_processed += 1\n",
    "    queries_processed += len(chunk_queries_data)\n",
    "    print(f\"‚úì Chunk results saved to database ({queries_processed} queries processed in this run)\\n\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "if TEST_MODE and chunks_processed >= MAX_CHUNKS_TEST:\n",
    "    print(f\"‚úì Test run completed! Processed {chunks_processed} chunks ({queries_processed} queries)\")\n",
    "    print(f\"   To process all queries, set TEST_MODE = False\")\n",
    "else:\n",
    "    print(\"‚úì All translations completed!\")\n",
    "\n",
    "# Show final stats\n",
    "final_stats = get_translation_stats()\n",
    "print(f\"\\n=== Final Database Stats ===\")\n",
    "print(f\"Total queries in DB: {final_stats['total']}\")\n",
    "print(f\"Successfully translated: {final_stats['success']}\")\n",
    "print(f\"Failed: {final_stats['failed']}\")\n",
    "print(f\"Other: {final_stats['other']}\")\n",
    "\n",
    "# Auto-export to CSV\n",
    "print(f\"\\nüì§ Exporting results to CSV...\")\n",
    "export_to_csv(output_csv_path)\n",
    "print(f\"‚úì Report file: {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# RECOVERY CELL - Re-fetch results by project ID\n",
    "# ========================================\n",
    "# Use this cell if the connection failed during translation\n",
    "# and you want to retrieve the results without re-translating\n",
    "\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration - Fill these in from the previous run\n",
    "RECOVERY_MODE = True  # Set to True to enable recovery\n",
    "RECOVERY_PROJECT_ID = 10017  # e.g., 10017\n",
    "RECOVERY_TRANSLATION_ID = \"27fda346-4635-48d2-ad58-d2db510461d2\"  # e.g., \"27fda346-4635-48d2-ad58-d2db510461d2\"\n",
    "RECOVERY_CHUNK_START = 0  # Which query index this chunk started at (from the error message)\n",
    "RECOVERY_CHUNK_SIZE = 10  # Size of the chunk being recovered\n",
    "\n",
    "input_csv_path = Path.home() / \"dma\" / \"dma-pearson-assessment\" / \"included_queries.csv\"\n",
    "output_csv_path = Path.home() / \"dma\" / \"dma-pearson-assessment\" / \"report.csv\"\n",
    "\n",
    "if RECOVERY_MODE:\n",
    "    if RECOVERY_PROJECT_ID is None or RECOVERY_TRANSLATION_ID is None:\n",
    "        print(\"‚ùå Error: Please set RECOVERY_PROJECT_ID and RECOVERY_TRANSLATION_ID\")\n",
    "    else:\n",
    "        print(f\"üîÑ Recovery Mode: Fetching results for project {RECOVERY_PROJECT_ID}, translation {RECOVERY_TRANSLATION_ID}\")\n",
    "        \n",
    "        from databricks_notebook import view_translation_results_as_dict, _get_current_api_key\n",
    "        \n",
    "        # Get API key (should already be set from previous cell)\n",
    "        api_key = _get_current_api_key(org_token, host)\n",
    "        \n",
    "        # Fetch the translation results\n",
    "        print(\"Fetching translation results...\")\n",
    "        translation_results = view_translation_results_as_dict(\n",
    "            api_key, \n",
    "            RECOVERY_PROJECT_ID, \n",
    "            RECOVERY_TRANSLATION_ID, \n",
    "            host\n",
    "        )\n",
    "        \n",
    "        translated_models = translation_results.get('translated_models', [])\n",
    "        print(f\"‚úì Retrieved {len(translated_models)} translation results\")\n",
    "        \n",
    "        # Display the results\n",
    "        print(\"\\n=== Translation Results ===\")\n",
    "        success_count = 0\n",
    "        failed_count = 0\n",
    "        other_count = 0\n",
    "        \n",
    "        for i, model in enumerate(translated_models, 1):\n",
    "            status = model.get('translation_status', 'unknown')\n",
    "            asset_name = model.get('asset_name', 'unnamed')\n",
    "            icon = '‚úÖ' if status == 'success' else '‚ö†Ô∏è' if status == 'failed' else '‚ùì'\n",
    "            print(f\"{i}. {icon} {asset_name}: {status}\")\n",
    "            \n",
    "            if status == 'success':\n",
    "                success_count += 1\n",
    "            elif status == 'failed':\n",
    "                failed_count += 1\n",
    "            else:\n",
    "                other_count += 1\n",
    "        \n",
    "        # Read the original queries to get query hashes\n",
    "        print(f\"\\nüîÑ Reading original CSV to match query hashes (chunk starting at index {RECOVERY_CHUNK_START})...\")\n",
    "        \n",
    "        queries_data = []\n",
    "        with open(input_csv_path, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for i, row in enumerate(reader):\n",
    "                # Only get the chunk we're recovering\n",
    "                if i >= RECOVERY_CHUNK_START and i < RECOVERY_CHUNK_START + RECOVERY_CHUNK_SIZE:\n",
    "                    queries_data.append({\n",
    "                        'query_hash': row['QueryHash'],\n",
    "                        'query_text': row['QueryText']\n",
    "                    })\n",
    "                if i >= RECOVERY_CHUNK_START + RECOVERY_CHUNK_SIZE:\n",
    "                    break\n",
    "        \n",
    "        print(f\"‚úì Matched {len(queries_data)} queries from CSV\")\n",
    "        \n",
    "        # Prepare report rows\n",
    "        chunk_report_rows = []\n",
    "        for i, model in enumerate(translated_models):\n",
    "            if i < len(queries_data):\n",
    "                query_data = queries_data[i]\n",
    "                status = model.get('translation_status', '')\n",
    "                \n",
    "                report_row = {\n",
    "                    'query_hash': query_data['query_hash'],\n",
    "                    'asset_name': model.get('asset_name', ''),\n",
    "                    'original_query': query_data['query_text'],\n",
    "                    'translation_status': status,\n",
    "                    'translation': model.get('target_sql', '')\n",
    "                }\n",
    "                chunk_report_rows.append(report_row)\n",
    "        \n",
    "        # Save to SQLite database\n",
    "        print(f\"\\nüíæ Saving results to database...\")\n",
    "        save_translation_results(chunk_report_rows, RECOVERY_PROJECT_ID, RECOVERY_TRANSLATION_ID)\n",
    "        print(f\"‚úì Successfully saved {len(chunk_report_rows)} rows to database\")\n",
    "        \n",
    "        # Export to CSV\n",
    "        print(f\"\\nüì§ Exporting to CSV: {output_csv_path}\")\n",
    "        export_to_csv(output_csv_path)\n",
    "        \n",
    "        print(f\"\\n=== Recovery Summary ===\")\n",
    "        print(f\"Total recovered: {len(chunk_report_rows)}\")\n",
    "        print(f\"Successfully translated: {success_count}\")\n",
    "        print(f\"Failed: {failed_count}\")\n",
    "        print(f\"Other: {other_count}\")\n",
    "        \n",
    "        # Show overall database stats\n",
    "        final_stats = get_translation_stats()\n",
    "        print(f\"\\n=== Overall Database Stats ===\")\n",
    "        print(f\"Total queries in DB: {final_stats['total']}\")\n",
    "        print(f\"Successfully translated: {final_stats['success']}\")\n",
    "        print(f\"Failed: {final_stats['failed']}\")\n",
    "        print(f\"Other: {final_stats['other']}\")\n",
    "        \n",
    "        print(\"\\nYou can access the full results in the 'translation_results' variable\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Recovery mode is disabled. Set RECOVERY_MODE = True to use this feature.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# UTILITY CELL - Database Queries & Export\n",
    "# ========================================\n",
    "# Use this cell to query the database and export results\n",
    "\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "\n",
    "output_csv_path = Path.home() / \"dma\" / \"dma-pearson-assessment\" / \"report.csv\"\n",
    "\n",
    "# === OPTION 1: View Statistics ===\n",
    "print(\"=== Database Statistics ===\")\n",
    "stats = get_translation_stats()\n",
    "print(f\"Total queries: {stats['total']}\")\n",
    "print(f\"Successfully translated: {stats['success']}\")\n",
    "print(f\"Failed: {stats['failed']}\")\n",
    "print(f\"Other statuses: {stats['other']}\")\n",
    "\n",
    "# === OPTION 2: Export to CSV ===\n",
    "print(f\"\\nüì§ Exporting to CSV: {output_csv_path}\")\n",
    "export_to_csv(output_csv_path)\n",
    "\n",
    "# === OPTION 3: Query specific records ===\n",
    "# Uncomment to use:\n",
    "# conn = sqlite3.connect(db_path)\n",
    "# cursor = conn.cursor()\n",
    "\n",
    "# # Get failed translations\n",
    "# cursor.execute(\"SELECT query_hash, asset_name, translation_status FROM translations WHERE translation_status = 'failed'\")\n",
    "# failed = cursor.fetchall()\n",
    "# print(f\"\\n=== Failed Translations ({len(failed)}) ===\")\n",
    "# for row in failed[:10]:  # Show first 10\n",
    "#     print(f\"  - {row[0]}: {row[1]} -> {row[2]}\")\n",
    "\n",
    "# # Get translations by status\n",
    "# cursor.execute(\"SELECT translation_status, COUNT(*) FROM translations GROUP BY translation_status\")\n",
    "# status_breakdown = cursor.fetchall()\n",
    "# print(f\"\\n=== Status Breakdown ===\")\n",
    "# for status, count in status_breakdown:\n",
    "#     print(f\"  {status}: {count}\")\n",
    "\n",
    "# conn.close()\n",
    "\n",
    "# === OPTION 4: Find untranslated queries ===\n",
    "# Uncomment to use:\n",
    "# input_csv_path = Path.home() / \"dma\" / \"dma-pearson-assessment\" / \"included_queries.csv\"\n",
    "# untranslated = get_untranslated_queries(input_csv_path)\n",
    "# print(f\"\\n=== Untranslated Queries ===\")\n",
    "# print(f\"Found {len(untranslated)} queries not yet in database\")\n",
    "# if len(untranslated) > 0:\n",
    "#     print(f\"First few: {untranslated[:5]}\")\n",
    "\n",
    "# === OPTION 5: View recent translations ===\n",
    "# Uncomment to use:\n",
    "# conn = sqlite3.connect(db_path)\n",
    "# cursor = conn.cursor()\n",
    "# cursor.execute(\"\"\"\n",
    "#     SELECT query_hash, asset_name, translation_status, updated_at \n",
    "#     FROM translations \n",
    "#     ORDER BY updated_at DESC \n",
    "#     LIMIT 10\n",
    "# \"\"\")\n",
    "# recent = cursor.fetchall()\n",
    "# print(f\"\\n=== Recent Translations ===\")\n",
    "# for row in recent:\n",
    "#     print(f\"  {row[0]}: {row[1]} -> {row[2]} (updated: {row[3]})\")\n",
    "# conn.close()\n",
    "\n",
    "print(\"\\n‚úì Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Recovery Mode: Fetching results for project 10017, translation 27fda346-4635-48d2-ad58-d2db510461d2\n",
      "Fetching translation results...\n",
      "‚úì Translation completed with status: done\n",
      "‚úì Retrieved 3 translation results\n",
      "\n",
      "=== Translation Results ===\n",
      "1. ‚ùì MXCI_SANDBOX.ASTRONOMY.problemxml_correct_answer: validation_pending\n",
      "2. ‚ùì temp_access_subscriptions: validation_pending\n",
      "3. ‚ùì NAIILS.SALES_CREDITING.OLP_SUBSCRIPTION_REPORT_2025_ONLY: validation_pending\n",
      "\n",
      "üîÑ Reading original CSV to match query hashes...\n",
      "‚úì Matched 10 queries from CSV\n",
      "\n",
      "üíæ Writing results to /Users/sergeyklinov/dma/dma-pearson-assessment/report.csv...\n",
      "‚úì Successfully wrote 3 rows to report\n",
      "\n",
      "=== Recovery Summary ===\n",
      "Total recovered: 3\n",
      "Successfully translated: 0\n",
      "Failed: 0\n",
      "Other: 3\n",
      "\n",
      "You can access the full results in the 'translation_results' variable\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# RECOVERY CELL - Re-fetch results by project ID\n",
    "# ========================================\n",
    "# Use this cell if the connection failed during translation\n",
    "# and you want to retrieve the results without re-translating\n",
    "\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration - Fill these in from the previous run\n",
    "RECOVERY_MODE = True  # Set to True to enable recovery\n",
    "RECOVERY_PROJECT_ID = 10017  # e.g., 10017\n",
    "RECOVERY_TRANSLATION_ID = \"27fda346-4635-48d2-ad58-d2db510461d2\"  # e.g., \"27fda346-4635-48d2-ad58-d2db510461d2\"\n",
    "RECOVERY_CHUNK_START = 0  # Which query index this chunk started at (from the error message)\n",
    "RECOVERY_CHUNK_SIZE = 10  # Size of the chunk being recovered\n",
    "\n",
    "output_csv_path = Path.home() / \"dma\" / \"dma-pearson-assessment\" / \"report.csv\"\n",
    "\n",
    "if RECOVERY_MODE:\n",
    "    if RECOVERY_PROJECT_ID is None or RECOVERY_TRANSLATION_ID is None:\n",
    "        print(\"‚ùå Error: Please set RECOVERY_PROJECT_ID and RECOVERY_TRANSLATION_ID\")\n",
    "    else:\n",
    "        print(f\"üîÑ Recovery Mode: Fetching results for project {RECOVERY_PROJECT_ID}, translation {RECOVERY_TRANSLATION_ID}\")\n",
    "        \n",
    "        from databricks_notebook import view_translation_results_as_dict, _get_current_api_key\n",
    "        \n",
    "        # Get API key (should already be set from previous cell)\n",
    "        api_key = _get_current_api_key(org_token, host)\n",
    "        \n",
    "        # Fetch the translation results\n",
    "        print(\"Fetching translation results...\")\n",
    "        translation_results = view_translation_results_as_dict(\n",
    "            api_key, \n",
    "            RECOVERY_PROJECT_ID, \n",
    "            RECOVERY_TRANSLATION_ID, \n",
    "            host\n",
    "        )\n",
    "        \n",
    "        translated_models = translation_results.get('translated_models', [])\n",
    "        print(f\"‚úì Retrieved {len(translated_models)} translation results\")\n",
    "        \n",
    "        # Display the results\n",
    "        print(\"\\n=== Translation Results ===\")\n",
    "        success_count = 0\n",
    "        failed_count = 0\n",
    "        other_count = 0\n",
    "        \n",
    "        for i, model in enumerate(translated_models, 1):\n",
    "            status = model.get('translation_status', 'unknown')\n",
    "            asset_name = model.get('asset_name', 'unnamed')\n",
    "            icon = '‚úÖ' if status == 'success' else '‚ö†Ô∏è' if status == 'failed' else '‚ùì'\n",
    "            print(f\"{i}. {icon} {asset_name}: {status}\")\n",
    "            \n",
    "            if status == 'success':\n",
    "                success_count += 1\n",
    "            elif status == 'failed':\n",
    "                failed_count += 1\n",
    "            else:\n",
    "                other_count += 1\n",
    "        \n",
    "        # Read the original queries to get query hashes\n",
    "        print(\"\\nüîÑ Reading original CSV to match query hashes...\")\n",
    "        input_csv_path = Path.home() / \"dma\" / \"dma-pearson-assessment\" / \"included_queries.csv\"\n",
    "        \n",
    "        queries_data = []\n",
    "        with open(input_csv_path, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for i, row in enumerate(reader):\n",
    "                # Only get the chunk we're recovering\n",
    "                if i >= RECOVERY_CHUNK_START and i < RECOVERY_CHUNK_START + RECOVERY_CHUNK_SIZE:\n",
    "                    queries_data.append({\n",
    "                        'query_hash': row['QueryHash'],\n",
    "                        'query_text': row['QueryText']\n",
    "                    })\n",
    "                if i >= RECOVERY_CHUNK_START + RECOVERY_CHUNK_SIZE:\n",
    "                    break\n",
    "        \n",
    "        print(f\"‚úì Matched {len(queries_data)} queries from CSV\")\n",
    "        \n",
    "        # Prepare report rows\n",
    "        chunk_report_rows = []\n",
    "        for i, model in enumerate(translated_models):\n",
    "            if i < len(queries_data):\n",
    "                query_data = queries_data[i]\n",
    "                status = model.get('translation_status', '')\n",
    "                \n",
    "                report_row = {\n",
    "                    'query_hash': query_data['query_hash'],\n",
    "                    'asset_name': model.get('asset_name', ''),\n",
    "                    'original_query': query_data['query_text'],\n",
    "                    'translation_status': status,\n",
    "                    'translation': model.get('target_sql', '')\n",
    "                }\n",
    "                chunk_report_rows.append(report_row)\n",
    "        \n",
    "        # Append to CSV file\n",
    "        print(f\"\\nüíæ Writing results to {output_csv_path}...\")\n",
    "        \n",
    "        # Check if file exists to determine if we need to write header\n",
    "        file_exists = output_csv_path.exists()\n",
    "        \n",
    "        with open(output_csv_path, 'a', newline='', encoding='utf-8') as f:\n",
    "            fieldnames = ['query_hash', 'asset_name', 'original_query', 'translation_status', 'translation']\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            \n",
    "            # Write header if file doesn't exist\n",
    "            if not file_exists:\n",
    "                writer.writeheader()\n",
    "            \n",
    "            writer.writerows(chunk_report_rows)\n",
    "        \n",
    "        print(f\"‚úì Successfully wrote {len(chunk_report_rows)} rows to report\")\n",
    "        print(f\"\\n=== Recovery Summary ===\")\n",
    "        print(f\"Total recovered: {len(chunk_report_rows)}\")\n",
    "        print(f\"Successfully translated: {success_count}\")\n",
    "        print(f\"Failed: {failed_count}\")\n",
    "        print(f\"Other: {other_count}\")\n",
    "        print(\"\\nYou can access the full results in the 'translation_results' variable\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Recovery mode is disabled. Set RECOVERY_MODE = True to use this feature.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Live Translations for Databricks",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
