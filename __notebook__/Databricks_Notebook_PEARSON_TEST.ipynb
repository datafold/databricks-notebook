{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing databricks-notebook from /Users/sergeyklinov/databricks-notebook\n",
      "Obtaining file:///Users/sergeyklinov/databricks-notebook\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: python-dotenv>=1.2.1 in /Users/sergeyklinov/databricks-notebook/.venv/lib/python3.12/site-packages (from databricks-notebook==0.1.0) (1.2.1)\n",
      "Building wheels for collected packages: databricks-notebook\n",
      "  Building editable for databricks-notebook (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for databricks-notebook: filename=databricks_notebook-0.1.0-0.editable-py3-none-any.whl size=2939 sha256=c97e1f72ab6a40e112e24b1a4740613802105aa28812cd18245f7ac8743c476d\n",
      "  Stored in directory: /private/var/folders/3y/p4yqdnw167xfr84r44_t60lh0000gn/T/pip-ephem-wheel-cache-ji9la71b/wheels/d6/fe/61/e1ee441d5c3d6bacd1e078d8335cf494301163e0f54e0a9d49\n",
      "Successfully built databricks-notebook\n",
      "Installing collected packages: databricks-notebook\n",
      "  Attempting uninstall: databricks-notebook\n",
      "    Found existing installation: databricks-notebook 0.1.0\n",
      "    Uninstalling databricks-notebook-0.1.0:\n",
      "      Successfully uninstalled databricks-notebook-0.1.0\n",
      "Successfully installed databricks-notebook-0.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Install dependencies\n",
    "LOCAL_DATABRICKS_NOTEBOOK_PATH = os.getenv('LOCAL_DATABRICKS_NOTEBOOK_PATH')\n",
    "if LOCAL_DATABRICKS_NOTEBOOK_PATH and pathlib.Path(LOCAL_DATABRICKS_NOTEBOOK_PATH).exists():\n",
    "    print(f\"Installing databricks-notebook from {LOCAL_DATABRICKS_NOTEBOOK_PATH}\")\n",
    "    %pip install --editable \"{LOCAL_DATABRICKS_NOTEBOOK_PATH}\"\n",
    "else:\n",
    "    print(\"Installing databricks-notebook from git\")\n",
    "    %pip install git+https://github.com/datafold/databricks-notebook.git\n",
    "\n",
    "# Restart to make dependencies available\n",
    "# %restart_python on databricks notebook\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_token = \"my_secret_token\" # do not change\n",
    "host=\"https://sergey.st.datafold.io\"\n",
    "identity = None\n",
    "\n",
    "# We collect basic identity information to help track and resolve any issues\n",
    "# with SQL translation and provide you with the best experience. This data is\n",
    "# used internally by Datafold only and helps us:\n",
    "# - Diagnose translation errors specific to your workspace configuration\n",
    "# - Improve translation quality based on real usage patterns\n",
    "# - Provide better support when you need assistance\n",
    "#\n",
    "# If you prefer not to share certain information, you can comment out specific\n",
    "# fields below or remove this entire code block. The tool will still work, but\n",
    "# we may have limited ability to help troubleshoot issues.\n",
    "\n",
    "# def get_context_info():\n",
    "#     context = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "#     return {\n",
    "#         'workspace_id': context.workspaceId().get(),\n",
    "#         'workspace_url': context.browserHostName().get(),\n",
    "#         'cluster_id': context.clusterId().get(),\n",
    "#         'notebook_path': context.notebookPath().get(),\n",
    "#         'user': context.userName().get()\n",
    "#     }\n",
    "\n",
    "# identity = get_context_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import csv\nimport os\nfrom pathlib import Path\nfrom databricks_notebook import translate_queries_and_get_results\n\n# Configuration\nCHUNK_SIZE = 100  # Process 100 queries at a time\nTEST_MODE = True  # Set to False to process all queries\nMAX_CHUNKS_TEST = 2  # Only process first 2 chunks in test mode\nOFFSET = 0  # Start from this query number (0 = start from beginning, 24 = start from 25th query)\n\ninput_csv_path = Path.home() / \"dma\" / \"dma-pearson-assessment\" / \"included_queries.csv\"\noutput_csv_path = Path.home() / \"dma\" / \"dma-pearson-assessment\" / \"report.csv\"\n\nprint(f\"Reading queries from: {input_csv_path}\")\n\n# Read all queries from CSV\nqueries_data = []\nwith open(input_csv_path, 'r', encoding='utf-8') as f:\n    reader = csv.DictReader(f)\n    for row in reader:\n        queries_data.append({\n            'query_hash': row['QueryHash'],\n            'query_text': row['QueryText']\n        })\n\ntotal_queries = len(queries_data)\nprint(f\"Found {total_queries} queries to translate\")\n\n# Apply offset\nif OFFSET > 0:\n    queries_data = queries_data[OFFSET:]\n    print(f\"⚠️  OFFSET SET: Starting from query #{OFFSET + 1} (skipping first {OFFSET} queries)\")\n    print(f\"   Remaining queries to process: {len(queries_data)}\")\n\nprint(f\"Processing in chunks of {CHUNK_SIZE}\")\n\nif TEST_MODE:\n    print(f\"\\n⚠️  TEST MODE ENABLED - Processing only first {MAX_CHUNKS_TEST} chunks ({MAX_CHUNKS_TEST * CHUNK_SIZE} queries max)\")\n    print(f\"   Set TEST_MODE = False to process all queries\\n\")\n\n# Initialize or clear the output CSV file with headers\nwith open(output_csv_path, 'w', newline='', encoding='utf-8') as f:\n    fieldnames = ['query_hash', 'asset_name', 'original_query', 'translation_status', 'translation']\n    writer = csv.DictWriter(f, fieldnames=fieldnames)\n    writer.writeheader()\n\nprint(f\"Output file initialized: {output_csv_path}\\n\")\n\n# Process queries in chunks\nsuccess_count = 0\nfailed_count = 0\nother_count = 0\nchunks_processed = 0\nqueries_processed = 0\n\nfor chunk_start in range(0, len(queries_data), CHUNK_SIZE):\n    # Stop after MAX_CHUNKS_TEST chunks if in test mode\n    if TEST_MODE and chunks_processed >= MAX_CHUNKS_TEST:\n        print(f\"\\n⚠️  TEST MODE: Stopping after {chunks_processed} chunks\")\n        break\n    \n    chunk_end = min(chunk_start + CHUNK_SIZE, len(queries_data))\n    chunk_queries_data = queries_data[chunk_start:chunk_end]\n    \n    actual_start = OFFSET + chunk_start + 1\n    actual_end = OFFSET + chunk_end\n    print(f\"=== Processing queries {actual_start} to {actual_end} of {total_queries} (total in file) ===\")\n    \n    # Extract query texts for this chunk\n    queries_to_translate = [q['query_text'] for q in chunk_queries_data]\n    \n    # Translate this chunk\n    print(f\"Translating {len(queries_to_translate)} queries...\")\n    translation_results = translate_queries_and_get_results(\n        queries_to_translate, \n        org_token, \n        identity, \n        host\n    )\n    \n    print(\"Translation completed for this chunk!\")\n    \n    # Prepare report data for this chunk\n    chunk_report_rows = []\n    translated_models = translation_results.get('translated_models', [])\n    \n    for i, query_data in enumerate(chunk_queries_data):\n        # Match the query with its translation result by index\n        if i < len(translated_models):\n            model = translated_models[i]\n            status = model.get('translation_status', '')\n            \n            report_row = {\n                'query_hash': query_data['query_hash'],\n                'asset_name': model.get('asset_name', ''),\n                'original_query': query_data['query_text'],\n                'translation_status': status,\n                'translation': model.get('target_sql', '')\n            }\n            \n            # Update counters\n            if status == 'success':\n                success_count += 1\n            elif status == 'failed':\n                failed_count += 1\n            else:\n                other_count += 1\n        else:\n            # In case there's a mismatch\n            report_row = {\n                'query_hash': query_data['query_hash'],\n                'asset_name': '',\n                'original_query': query_data['query_text'],\n                'translation_status': 'not_translated',\n                'translation': ''\n            }\n            other_count += 1\n        \n        chunk_report_rows.append(report_row)\n    \n    # Append this chunk's results to the CSV file\n    with open(output_csv_path, 'a', newline='', encoding='utf-8') as f:\n        fieldnames = ['query_hash', 'asset_name', 'original_query', 'translation_status', 'translation']\n        writer = csv.DictWriter(f, fieldnames=fieldnames)\n        writer.writerows(chunk_report_rows)\n    \n    chunks_processed += 1\n    queries_processed += len(chunk_queries_data)\n    print(f\"✓ Chunk results written to report ({queries_processed} queries processed so far)\\n\")\n\nprint(\"=\" * 50)\nif TEST_MODE and chunks_processed >= MAX_CHUNKS_TEST:\n    print(f\"✓ Test run completed! Processed {chunks_processed} chunks ({queries_processed} queries)\")\n    print(f\"   To process all queries, set TEST_MODE = False\")\nelse:\n    print(\"✓ All translations completed!\")\nprint(f\"✓ Report file: {output_csv_path}\")\nprint(\"\\n=== Translation Summary ===\")\nprint(f\"Queries processed: {queries_processed}\")\nprint(f\"Successfully translated: {success_count}\")\nprint(f\"Failed: {failed_count}\")\nprint(f\"Other: {other_count}\")"
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Live Translations for Databricks",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}